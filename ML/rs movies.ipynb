{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('..\\clean\\movie_after_cleaning.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf1\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      The Shawshank Redemption\n",
       "1                     12th Fail\n",
       "2                 The Godfather\n",
       "3               The Dark Knight\n",
       "4              Schindler's List\n",
       "                 ...           \n",
       "496             My Name Is Khan\n",
       "497                       Amour\n",
       "498                       Laura\n",
       "499                     Head-On\n",
       "500       Cat on a Hot Tin Roof\n",
       "Name: title, Length: 501, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles = movies['title'].iloc[0:501]\n",
    "movie_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Title  \\\n",
      "0    The Shawshank Redemption   \n",
      "1                   12th Fail   \n",
      "2               The Godfather   \n",
      "3             The Dark Knight   \n",
      "4            Schindler's List   \n",
      "..                        ...   \n",
      "496           My Name Is Khan   \n",
      "497                     Amour   \n",
      "498                     Laura   \n",
      "499                   Head-On   \n",
      "500     Cat on a Hot Tin Roof   \n",
      "\n",
      "                                Second Section Content  \n",
      "0    In early 1947, Portland, Maine, banker Andy Du...  \n",
      "1    Born in dacoit-infested Chambal, Manoj Kumar S...  \n",
      "2    In 1945, the New York City Corleone family don...  \n",
      "3    A gang of masked criminals rob a mafia-owned b...  \n",
      "4    In Kraków during World War II, the Nazis force...  \n",
      "..                                                 ...  \n",
      "496  Rizwan Khan, a Muslim, grew up with his brothe...  \n",
      "497  \"Amour, Amour\", a song by Plastic Bertrand\\nD'...  \n",
      "498  Laura (given name), including lists of people ...  \n",
      "499  Head On (1980 film), a Canadian drama film\\nHe...  \n",
      "500  A family in the American South is in crisis, e...  \n",
      "\n",
      "[501 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "movie_titles = movies['title'].iloc[0:501]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Sample movie titles\n",
    "\n",
    "# Create Wikipedia object with a custom user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='Your User Agent')\n",
    "\n",
    "# Function to fetch plot from Wikipedia based on title\n",
    "\n",
    "# Function to fetch content of the second section from Wikipedia based on title\n",
    "def get_second_section_content(title):\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        sections = page.sections\n",
    "        if len(sections) > 0:\n",
    "            return sections[0].text\n",
    "        else:\n",
    "            return \"Second section not found\"\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "# Create DataFrame\n",
    "df1 = pd.DataFrame({'Title': movie_titles})\n",
    "\n",
    "# Apply function to fetch content of the second section for each title and create new column 'Second Section Content'\n",
    "df1['Second Section Content'] = df1['Title'].apply(get_second_section_content)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Title  \\\n",
      "501             Arsenic and Old Lace   \n",
      "502           Miracle on 34th Street   \n",
      "503                      Talk to Her   \n",
      "504     The Adventures of Robin Hood   \n",
      "505                    Kal Ho Naa Ho   \n",
      "...                              ...   \n",
      "996              A Fish Called Wanda   \n",
      "997   Fear and Loathing in Las Vegas   \n",
      "998   An American Werewolf in London   \n",
      "999                           Lolita   \n",
      "1000             Kiss Kiss Bang Bang   \n",
      "\n",
      "                                 Second Section Content  \n",
      "501                            Second section not found  \n",
      "502   On the morning of the Macy's Thanksgiving Day ...  \n",
      "503   The story unfolds in flashbacks, giving detail...  \n",
      "504   Richard, the Norman King of England, is taken ...  \n",
      "505   Naina Catherine Kapur is a pessimistic, uptigh...  \n",
      "...                                                 ...  \n",
      "996   London-based gangster George Thomason plans a ...  \n",
      "997   The novel Fear and Loathing in Las Vegas is ba...  \n",
      "998   Two American graduate students from New York C...  \n",
      "999   The novel is prefaced by a fictitious foreword...  \n",
      "1000  Harry Lockhart unintentionally wins a screen t...  \n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "movie_titles = movies['title'].iloc[501:1001]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Sample movie titles\n",
    "\n",
    "# Create Wikipedia object with a custom user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='Your User Agent')\n",
    "\n",
    "# Function to fetch plot from Wikipedia based on title\n",
    "\n",
    "# Function to fetch content of the second section from Wikipedia based on title\n",
    "def get_second_section_content(title):\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        sections = page.sections\n",
    "        if len(sections) > 0:\n",
    "            return sections[0].text\n",
    "        else:\n",
    "            return \"Second section not found\"\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "# Create DataFrame\n",
    "df2 = pd.DataFrame({'Title': movie_titles})\n",
    "\n",
    "# Apply function to fetch content of the second section for each title and create new column 'Second Section Content'\n",
    "df2['Second Section Content'] = df2['Title'].apply(get_second_section_content)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Title  \\\n",
      "1501                     Lawless   \n",
      "1502                  Caddyshack   \n",
      "1503                       Carol   \n",
      "1504        The Age of Innocence   \n",
      "1505         The Prince of Egypt   \n",
      "...                          ...   \n",
      "1996           Love and Monsters   \n",
      "1997     Solo: A Star Wars Story   \n",
      "1998                  Cinderella   \n",
      "1999         Conan the Barbarian   \n",
      "2000  Ace Ventura: Pet Detective   \n",
      "\n",
      "                                 Second Section Content  \n",
      "1501  Lawless (British TV series), a 2004 TV miniser...  \n",
      "1502  High school student Danny Noonan is conflicted...  \n",
      "1503  Carol (given name)\\nHenri Carol (1910–1984), F...  \n",
      "1504  The Age of Innocence, which was set in the tim...  \n",
      "1505  In Ancient Egypt, the enslaved Hebrew people p...  \n",
      "...                                                 ...  \n",
      "1996                      Love Monster (disambiguation)  \n",
      "1997  On the planet Corellia, orphans Han and Qi'ra ...  \n",
      "1998                                                     \n",
      "1999  Robert E. Howard created Conan the Barbarian i...  \n",
      "2000  Ace Ventura, an eccentric and offbeat private ...  \n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "movie_titles = movies['title'].iloc[1501:2001]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Sample movie titles\n",
    "\n",
    "# Create Wikipedia object with a custom user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='Your User Agent')\n",
    "\n",
    "# Function to fetch plot from Wikipedia based on title\n",
    "\n",
    "# Function to fetch content of the second section from Wikipedia based on title\n",
    "def get_second_section_content(title):\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        sections = page.sections\n",
    "        if len(sections) > 0:\n",
    "            return sections[0].text\n",
    "        else:\n",
    "            return \"Second section not found\"\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "# Create DataFrame\n",
    "df3 = pd.DataFrame({'Title': movie_titles})\n",
    "\n",
    "# Apply function to fetch content of the second section for each title and create new column 'Second Section Content'\n",
    "df3['Second Section Content'] = df3['Title'].apply(get_second_section_content)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title  \\\n",
      "1001            tick, tick... BOOM!   \n",
      "1002                      The Piano   \n",
      "1003                           Blow   \n",
      "1004  The Last Temptation of Christ   \n",
      "1005                             42   \n",
      "...                             ...   \n",
      "1496          A Quiet Place Part II   \n",
      "1497                    I Am Legend   \n",
      "1498        The Man from U.N.C.L.E.   \n",
      "1499                     The Outfit   \n",
      "1500            10 Cloverfield Lane   \n",
      "\n",
      "                                 Second Section Content  \n",
      "1001  The show was first performed as a workshop bet...  \n",
      "1002  In the mid-1800s, a Scotswoman named Ada McGra...  \n",
      "1003                     Blew (surname)\\nBlow (surname)  \n",
      "1004  Temptation of Christ, an event depicted in the...  \n",
      "1005  42 (dominoes), a game\\n42 (film), a 2013 biopi...  \n",
      "...                                                 ...  \n",
      "1496  During a Little League Baseball game in the sm...  \n",
      "1497                           Second section not found  \n",
      "1498  The series consists of 105 episodes originally...  \n",
      "1499  The Outfit (1973 film), a crime film starring ...  \n",
      "1500  In March 2016, after an argument with her fian...  \n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "movie_titles = movies['title'].iloc[1001:1501]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Sample movie titles\n",
    "\n",
    "# Create Wikipedia object with a custom user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='Your User Agent')\n",
    "\n",
    "# Function to fetch plot from Wikipedia based on title\n",
    "\n",
    "# Function to fetch content of the second section from Wikipedia based on title\n",
    "def get_second_section_content(title):\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        sections = page.sections\n",
    "        if len(sections) > 0:\n",
    "            return sections[0].text\n",
    "        else:\n",
    "            return \"Second section not found\"\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "# Create DataFrame\n",
    "df4 = pd.DataFrame({'Title': movie_titles})\n",
    "\n",
    "# Apply function to fetch content of the second section for each title and create new column 'Second Section Content'\n",
    "df4['Second Section Content'] = df4['Title'].apply(get_second_section_content)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Title  \\\n",
      "2001                    The Magnificent Seven   \n",
      "2002                  Mission: Impossible III   \n",
      "2003                              Vanilla Sky   \n",
      "2004  Star Wars: Episode VIII - The Last Jedi   \n",
      "2005                                 The Mask   \n",
      "...                                       ...   \n",
      "2496                              The Faculty   \n",
      "2497                            Weird Science   \n",
      "2498                                    Burnt   \n",
      "2499                              The Village   \n",
      "2500                    Nymphomaniac: Vol. II   \n",
      "\n",
      "                                 Second Section Content  \n",
      "2001  A gang of bandits led by Calvera periodically ...  \n",
      "2002  IMF agent Ethan Hunt is retired from fieldwork...  \n",
      "2003  David Aames, the owner of a large publishing c...  \n",
      "2004  Shortly after the battle of Starkiller Base, G...  \n",
      "2005                                                     \n",
      "...                                                 ...  \n",
      "2496  One evening at Herrington High School in Ohio,...  \n",
      "2497  Pseudoscience\\nWeerd Science, pseudonym for ra...  \n",
      "2498  Burning (disambiguation)\\nBurn (disambiguation...  \n",
      "2499  Village (Taiwan), the basic unit of Taiwanese ...  \n",
      "2500                                     Page not found  \n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "movie_titles = movies['title'].iloc[2001:2501]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Sample movie titles\n",
    "\n",
    "# Create Wikipedia object with a custom user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='Your User Agent')\n",
    "\n",
    "# Function to fetch plot from Wikipedia based on title\n",
    "\n",
    "# Function to fetch content of the second section from Wikipedia based on title\n",
    "def get_second_section_content(title):\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        sections = page.sections\n",
    "        if len(sections) > 0:\n",
    "            return sections[0].text\n",
    "        else:\n",
    "            return \"Second section not found\"\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "# Create DataFrame\n",
    "df5 = pd.DataFrame({'Title': movie_titles})\n",
    "\n",
    "# Apply function to fetch content of the second section for each title and create new column 'Second Section Content'\n",
    "df5['Second Section Content'] = df5['Title'].apply(get_second_section_content)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwikipediaapi\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m movie_titles \u001b[38;5;241m=\u001b[39m \u001b[43mmovies\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m2501\u001b[39m:\u001b[38;5;241m3001\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwikipediaapi\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "movie_titles = movies['title'].iloc[2501:3001]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Sample movie titles\n",
    "\n",
    "# Create Wikipedia object with a custom user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='User Agent')\n",
    "\n",
    "# Function to fetch plot from Wikipedia based on title\n",
    "\n",
    "# Function to fetch content of the second section from Wikipedia based on title\n",
    "def get_second_section_content(title):\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        sections = page.sections\n",
    "        if len(sections) > 0:\n",
    "            return sections[0].text\n",
    "        else:\n",
    "            return \"Second section not found\"\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "# Create DataFrame\n",
    "df6 = pd.DataFrame({'Title': movie_titles})\n",
    "\n",
    "# Apply function to fetch content of the second section for each title and create new column 'Second Section Content'\n",
    "df6['Second Section Content'] = df6['Title'].apply(get_second_section_content)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf6\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df6' is not defined"
     ]
    }
   ],
   "source": [
    "print(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[\"word_cloud\"]=movies[\"description\"]+\"\"+movies[\"genre\"]\n",
    "movies[\"title\"]=movies[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movies_splits[\"country\"][\"country\"].groupby(movies_splits[\"country\"][\"country\"]).count().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('./movie_after_cleaning.csv')\n",
    "movies_splits = pd.read_excel(\"./splits_movie.xlsx\", sheet_name=None)\n",
    "series = pd.read_csv('./series_after_cleaning.csv')\n",
    "series_splits = pd.read_excel(\"./splits_series.xlsx\", sheet_name=None)\n",
    "\n",
    "numofcountries=len(movies_splits[\"country\"][\"country\"].groupby(movies_splits[\"country\"][\"country\"]).count().sort_values(ascending=False).index)\n",
    "numoflang=len(movies_splits[\"language\"][\"language\"].groupby(movies_splits[\"language\"][\"language\"]).count().sort_values(ascending=False).index)\n",
    "numofmoives=movies.shape[0]+series.shape[0]\n",
    "avgvotes=int(movies[\"votes\"].mean()+series[\"votes\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countriesmovies=movies_splits[\"country\"][\"country\"].groupby(movies_splits[\"country\"][\"country\"]).count().sort_values(ascending=False).index \n",
    "countrieseries=series_splits[\"country\"][\"country\"].groupby(movies_splits[\"country\"][\"country\"]).count().sort_values(ascending=False).index \n",
    "numofcountries=len(countriesmovies.append(countrieseries).unique())\n",
    "\n",
    "languagesmovies=movies_splits[\"language\"][\"language\"].groupby(movies_splits[\"language\"][\"language\"]).count().sort_values(ascending=False).index\n",
    "languageseries=series_splits[\"language\"][\"language\"].groupby(movies_splits[\"language\"][\"language\"]).count().sort_values(ascending=False).index\n",
    "numoflang=len(languagesmovies.append(languageseries).unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160770"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgvotes=int((movies[\"votes\"].mean()+series[\"votes\"].mean())/2)\n",
    "avgvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[\"word_cloud\"]=movies[\"description\"]+\" \"+movies[\"genre\"]+\" \"+movies[\"director\"]+\" \"+movies[\"writer\"]+\" \"+movies[\"country\"]\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "movies[\"word_cloud\"] = movies[\"word_cloud\"].fillna('')  \n",
    "tfidf_matrix = tfidf.fit_transform(movies['word_cloud'])\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:7]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return movies['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[\"word_cloud\"]=movies[\"description\"]+\" \"+movies[\"genre\"]+\" \"+movies[\"director\"]+\" \"+movies[\"writer\"]+\" \"+movies[\"country\"]\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "movies[\"word_cloud\"] = movies[\"word_cloud\"].fillna('')  \n",
    "tfidf_matrix = tfidf.fit_transform(movies['word_cloud'])\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:7]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return movies['title'].iloc[movie_indices]\n",
    "x=[]\n",
    "for i in range(0,6):\n",
    "    x.append(movies[movies[\"title\"]==get_recommendations('inception').iloc[i]][\"link\"])\n",
    "image_urls=[]\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "for i in x:\n",
    "    driver = uc.Chrome()\n",
    "    driver.get(i.values[0])\n",
    "\n",
    "    image = driver.find_element( \"xpath\",\"//section[1]/section//div[contains(@class,'poster')]/img\")\n",
    "    image_urls.append(image.get_attribute(\"src\"))\n",
    "\n",
    "    driver.quit()\n",
    "import dash\n",
    "from dash import html\n",
    "\n",
    "# List of sample image URLs\n",
    "\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the layout of the app\n",
    "app.layout = html.Div([\n",
    "    # Image container\n",
    "    html.Div([\n",
    "        # Generate HTML elements for each image URL\n",
    "        html.Img(src=image_url, style={'width': '300px', 'height': '300px'}) \n",
    "        for image_url in image_urls\n",
    "    ], style={'display': 'flex', 'flexWrap': 'wrap', 'justifyContent': 'center'})\n",
    "])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20d381fc710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Input, Output\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the layout of the app\n",
    "app.layout = html.Div([\n",
    "    # Input field for movie title\n",
    "    dcc.Input(id='movie-input', type='text', placeholder='Enter a movie title...'),\n",
    "    \n",
    "    # Image container\n",
    "    html.Div(id='image-container', style={'display': 'flex', 'flexWrap': 'wrap', 'justifyContent': 'center'})\n",
    "])\n",
    "\n",
    "# Function to get recommendations\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:7]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return movies['title'].iloc[movie_indices]\n",
    "\n",
    "# Callback to update image container based on user input\n",
    "@app.callback(\n",
    "    Output('image-container', 'children'),\n",
    "    [Input('movie-input', 'value')]\n",
    ")\n",
    "def update_image_container(input_value):\n",
    "    if input_value:\n",
    "        x = []\n",
    "        for i in range(0, 6):\n",
    "            x.append(movies[movies[\"title\"] == get_recommendations(input_value).iloc[i]][\"link\"])\n",
    "        \n",
    "        image_urls = []\n",
    "        for i in x:\n",
    "            driver = uc.Chrome(headless=True)\n",
    "            driver.get(i.values[0])\n",
    "\n",
    "            image = driver.find_element(\"xpath\", \"//section[1]/section//div[contains(@class,'poster')]/img\")\n",
    "            image_urls.append(image.get_attribute(\"src\"))\n",
    "\n",
    "            driver.quit()\n",
    "        \n",
    "        # Generate HTML elements for each image URL\n",
    "        images = [html.Img(src=image_url, style={'width': '300px', 'height': '300px'}) for image_url in image_urls]\n",
    "        return images\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations2(description,tfidf_matrix):\n",
    "\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    tfidf_matrix2 = tfidf.transform(description)\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix2)\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(cosine_sim, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return movies['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45    https://www.imdb.com/title/tt0482571/?ref_=sr_...\n",
       " Name: link, dtype: object,\n",
       " 1019    https://www.imdb.com/title/tt0154506/?ref_=sr_...\n",
       " Name: link, dtype: object,\n",
       " 68    https://www.imdb.com/title/tt0209144/?ref_=sr_...\n",
       " Name: link, dtype: object,\n",
       " 15    https://www.imdb.com/title/tt0816692/?ref_=sr_...\n",
       " Name: link, dtype: object]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls=[]\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "for i in x:\n",
    "    driver = uc.Chrome()\n",
    "    driver.get(i.values[0])\n",
    "\n",
    "    image = driver.find_element( \"xpath\",\"//section[1]/section//div[contains(@class,'poster')]/img\")\n",
    "    image_urls.append(image.get_attribute(\"src\"))\n",
    "\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the layout of the app\n",
    "app.layout = dbc.Container([\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            dcc.Input(id='movie-input', type='text', placeholder='Enter movie title...')\n",
    "        ], width=6, className='mb-3'),\n",
    "        dbc.Col([\n",
    "            dbc.Button('Submit', id='submit-button', color='primary', className='mr-1')\n",
    "        ], width=3)\n",
    "    ], justify='center'),\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.Div(id='output')\n",
    "        ], width=6)\n",
    "    ], className='mt-3')\n",
    "])\n",
    "\n",
    "# Define callback to save user input into a variable\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('output', 'children'),\n",
    "    [dash.dependencies.Input('submit-button', 'n_clicks')],\n",
    "    [dash.dependencies.State('movie-input', 'value')]\n",
    ")\n",
    "def update_output(n_clicks, movie_title):\n",
    "    if n_clicks is not None:\n",
    "        return f'User input: {movie_title}'\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20d34870ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import html\n",
    "\n",
    "# List of sample image URLs\n",
    "\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the layout of the app\n",
    "app.layout = html.Div([\n",
    "    # Image container\n",
    "    html.Div([\n",
    "        # Generate HTML elements for each image URL\n",
    "        html.Img(src=image_url, style={'width': '300px', 'height': '300px'}) \n",
    "        for image_url in image_urls\n",
    "    ], style={'display': 'flex', 'flexWrap': 'wrap', 'justifyContent': 'center'})\n",
    "])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20d37666550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define function to load data based on tab selection\n",
    "def load_data(tab):\n",
    "    if tab == 'movie':\n",
    "        movie = pd.read_csv('..\\clean\\movie_after_cleaning.csv')\n",
    "        splits = pd.read_excel(\"..\\clean\\splits_movie.xlsx\", sheet_name=None)\n",
    "    elif tab == 'series':\n",
    "        movie = pd.read_csv('..\\clean\\series_after_cleaning.csv')\n",
    "        splits = pd.read_excel(\"..\\clean\\splits_series.xlsx\", sheet_name=None)\n",
    "    return movie, splits\n",
    "\n",
    "# Define visualizations\n",
    "def generate_visualizations(series, splits):\n",
    "    top_five_genres = series[\"parentalguide\"].value_counts().head(10).reset_index(name='count')\n",
    "    fig_treemap = px.treemap(top_five_genres, \n",
    "                             path=['parentalguide'],  \n",
    "                             values='count', \n",
    "                             title='Top Five Parental Guides - Treemap',\n",
    "                             color='count',color_continuous_scale='viridis')\n",
    "    fig_treemap.update_layout(template='plotly_dark', font=dict(color='yellow'))\n",
    "\n",
    "    text_data = ' '.join(series['description'].astype(str))\n",
    "    wordcloud = WordCloud(width=687, height=450, background_color='black').generate(text_data)\n",
    "    wordcloud_path = 'wordcloud.png'\n",
    "    wordcloud.to_file(wordcloud_path)\n",
    "\n",
    "    fig_wordcloud = html.Img(src=wordcloud_path)\n",
    "\n",
    "    top_values_language = splits[\"genre\"][\"genre\"].value_counts().head(10).reset_index(name='count')\n",
    "    total_count_language = top_values_language['count'].sum()\n",
    "    top_values_language['percentage'] = (top_values_language['count'] / total_count_language) * 100\n",
    "    fig_bar_language = px.bar(top_values_language, x='count', y=\"genre\", orientation='h',\n",
    "                              color='count', text='percentage',\n",
    "                              title='Top genres',\n",
    "                              labels={'count': 'Count', 'index': 'genre', 'percentage': 'Percentage'},\n",
    "                              color_continuous_scale='Viridis')\n",
    "    fig_bar_language.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
    "    fig_bar_language.update_layout(yaxis=dict(categoryorder='total ascending'))\n",
    "    fig_bar_language.update_layout(template='plotly_dark', font=dict(color='yellow'))\n",
    "\n",
    "    top_countries = splits[\"country\"][\"country\"].value_counts().head(30).reset_index(name='count')\n",
    "    country_mapping = {\n",
    "        'United States': 'USA',\n",
    "        'United Kingdom': 'GBR',\n",
    "        'France': 'FRA',\n",
    "        'Canada': 'CAN',\n",
    "        'Germany': 'DEU',\n",
    "        'Japan': 'JPN',\n",
    "        'India': 'IND',\n",
    "        'Australia': 'AUS',\n",
    "        'China': 'CHN',\n",
    "        'Italy': 'ITA',\n",
    "        'Spain': 'ESP',\n",
    "        'Mexico': 'MEX',\n",
    "        'Hong Kong': 'HKG',\n",
    "        'Sweden': 'SWE',\n",
    "        'Denmark': 'DNK',\n",
    "        'New Zealand': 'NZL',\n",
    "        'Belgium': 'BEL',\n",
    "        'South Korea': 'KOR',\n",
    "        'Ireland': 'IRL',\n",
    "        'Czech Republic': 'CZE',\n",
    "        'Switzerland': 'CHE',\n",
    "        'Hungary': 'HUN',\n",
    "        'Norway': 'NOR',\n",
    "        'United Arab Emirates': 'ARE',\n",
    "        'Netherlands': 'NLD',\n",
    "        'South Africa': 'ZAF',\n",
    "        'Poland': 'POL',\n",
    "        'West Germany': 'DEU',  # Assuming you want to use 'DEU' for Germany\n",
    "        'Austria': 'AUT',\n",
    "        'Turkey': 'TUR'\n",
    "    }\n",
    "    # Assuming 'df' is your DataFrame\n",
    "    top_countries['country'] = top_countries['country'].map(country_mapping)\n",
    "\n",
    "    fig_choropleth = px.choropleth(top_countries, \n",
    "                                    locations=\"country\",\n",
    "                                    color=\"count\",\n",
    "                                    hover_name=\"country\",\n",
    "                                    title=\"Choropleth Map of User Percent by Country\",\n",
    "                                    projection=\"natural earth\",\n",
    "                                    color_continuous_scale='Viridis')\n",
    "    fig_choropleth.update_layout(template='plotly_dark', font=dict(color='yellow'))\n",
    "\n",
    "    return fig_treemap, fig_wordcloud, fig_bar_language, fig_choropleth\n",
    "\n",
    "# Define the layout of the app\n",
    "app.layout = html.Div([\n",
    "    dcc.Tabs(id='tabs', value='movie', children=[\n",
    "        dcc.Tab(label='Movie', value='movie', style={'backgroundColor': 'black', 'color': 'yellow'}),\n",
    "        dcc.Tab(label='Series', value='series', style={'backgroundColor': 'black', 'color': 'yellow'}),\n",
    "    ]),\n",
    "    html.Div(id='tabs-content')\n",
    "])\n",
    "\n",
    "# Define callback to update visualizations based on tab selection\n",
    "@app.callback(\n",
    "    Output('tabs-content', 'children'),\n",
    "    [Input('tabs', 'value')]\n",
    ")\n",
    "def update_tab(tab):\n",
    "    series, splits = load_data(tab)\n",
    "    fig_treemap, fig_wordcloud, fig_bar_language, fig_choropleth = generate_visualizations(series, splits)\n",
    "    \n",
    "    if tab == 'movie':\n",
    "        return html.Div([\n",
    "            html.Div([\n",
    "                dcc.Graph(id='graph1', figure=fig_treemap),\n",
    "            ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "            html.Div([\n",
    "                fig_wordcloud,\n",
    "            ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "            html.Div([\n",
    "                dcc.Graph(id='graph3', figure=fig_bar_language),\n",
    "            ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "            html.Div([\n",
    "                dcc.Graph(id='graph4', figure=fig_choropleth),\n",
    "            ], style={'width': '50%', 'display': 'inline-block'})\n",
    "        ])\n",
    "    elif tab == 'series':\n",
    "        return html.Div([\n",
    "            html.Div([\n",
    "                dcc.Graph(id='graph1', figure=fig_treemap),\n",
    "            ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "            html.Div([\n",
    "                fig_wordcloud,\n",
    "            ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "            html.Div([\n",
    "                dcc.Graph(id='graph3', figure=fig_bar_language),\n",
    "            ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "            html.Div([\n",
    "                dcc.Graph(id='graph4', figure=fig_choropleth),\n",
    "            ], style={'width': '50%', 'display': 'inline-block'})\n",
    "        ])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "driver = uc.Chrome()\n",
    "driver.get('https://www.imdb.com/title/tt0111161/?ref_=nv_sr_srsg_0_tt_4_nm_4_q_shaws')\n",
    "\n",
    "image = driver.find_element( \"xpath\",\"//section[1]/section//div[contains(@class,'poster')]/img\")\n",
    "imagepath=image.get_attribute(\"src\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Address 'http://127.0.0.1:8050' already in use.\n    Try passing a different port to run_server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Run the app\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dash\\dash.py:2223\u001b[0m, in \u001b[0;36mDash.run_server\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_server\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"`run_server` is a deprecated alias of `run` and may be removed in a\u001b[39;00m\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;124;03m    future version. We recommend using `app.run` instead.\u001b[39;00m\n\u001b[0;32m   2220\u001b[0m \n\u001b[0;32m   2221\u001b[0m \u001b[38;5;124;03m    See `app.run` for usage information.\u001b[39;00m\n\u001b[0;32m   2222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2223\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dash\\dash.py:2114\u001b[0m, in \u001b[0;36mDash.run\u001b[1;34m(self, host, port, proxy, debug, jupyter_mode, jupyter_width, jupyter_height, jupyter_server_url, dev_tools_ui, dev_tools_props_check, dev_tools_serve_dev_bundles, dev_tools_hot_reload, dev_tools_hot_reload_interval, dev_tools_hot_reload_watch_interval, dev_tools_hot_reload_max_retry, dev_tools_silence_routes_logging, dev_tools_prune_errors, **flask_run_options)\u001b[0m\n\u001b[0;32m   2111\u001b[0m             extra_files\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m   2113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jupyter_dash\u001b[38;5;241m.\u001b[39mactive:\n\u001b[1;32m-> 2114\u001b[0m     \u001b[43mjupyter_dash\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_app\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_server_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun(host\u001b[38;5;241m=\u001b[39mhost, port\u001b[38;5;241m=\u001b[39mport, debug\u001b[38;5;241m=\u001b[39mdebug, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflask_run_options)\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dash\\_jupyter.py:404\u001b[0m, in \u001b[0;36mJupyterDash.run_app\u001b[1;34m(self, app, mode, width, height, host, port, server_url)\u001b[0m\n\u001b[0;32m    402\u001b[0m     display(HTML(msg))\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_error\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dash\\_jupyter.py:391\u001b[0m, in \u001b[0;36mJupyterDash.run_app\u001b[1;34m(self, app, mode, width, height, host, port, server_url)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[43mwait_for_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_colab:\n\u001b[0;32m    394\u001b[0m         JupyterDash\u001b[38;5;241m.\u001b[39m_display_in_colab(dashboard_url, port, mode, width, height)\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\retrying.py:56\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@six\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrying\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\retrying.py:266\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop(attempt_number, delay_since_first_attempt_ms):\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_exception \u001b[38;5;129;01mand\u001b[39;00m attempt\u001b[38;5;241m.\u001b[39mhas_exception:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;66;03m# get() on an attempt with an exception should cause it to be raised, but raise just in case\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mattempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RetryError(attempt)\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\retrying.py:301\u001b[0m, in \u001b[0;36mAttempt.get\u001b[1;34m(self, wrap_exception)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RetryError(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m         \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\retrying.py:251\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_attempts(attempt_number)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m Attempt(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32mc:\\Users\\mohamed elsayed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dash\\_jupyter.py:382\u001b[0m, in \u001b[0;36mJupyterDash.run_app.<locals>.wait_for_app\u001b[1;34m()\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    381\u001b[0m         url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 382\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAddress \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already in use.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Try passing a different port to run_server.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m         )\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mConnectionError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    387\u001b[0m     _get_error()\n",
      "\u001b[1;31mOSError\u001b[0m: Address 'http://127.0.0.1:8050' already in use.\n    Try passing a different port to run_server."
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': ['Movie 1', 'Movie 2', 'Movie 3', 'Movie 4'],\n",
    "    'Genre': ['Action', 'Comedy', 'Drama', 'Sci-Fi'],\n",
    "    'Rating': [8.5, 7.9, 6.8, 9.2]\n",
    "})\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the layout of the app\n",
    "app.layout = html.Div([\n",
    "    dcc.Dropdown(\n",
    "        id='genre-dropdown',\n",
    "        options=[{'label': genre, 'value': genre} for genre in df['Genre'].unique()],\n",
    "        value=None,\n",
    "        placeholder=\"Select a genre\"\n",
    "    ),\n",
    "    dcc.Slider(\n",
    "        id='rating-slider',\n",
    "        min=0,\n",
    "        max=10,\n",
    "        step=0.1,\n",
    "        marks={i: str(i) for i in range(0, 11)},\n",
    "        value=5,\n",
    "        tooltip={'placement': 'bottom', 'always_visible': True}\n",
    "    ),\n",
    "    html.Div(id='output')\n",
    "])\n",
    "\n",
    "# Define callback to update output\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('output', 'children'),\n",
    "    [dash.dependencies.Input('genre-dropdown', 'value'),\n",
    "     dash.dependencies.Input('rating-slider', 'value')]\n",
    ")\n",
    "def update_output(selected_genre, selected_rating):\n",
    "    if selected_genre:\n",
    "        filtered_df = df[df['Genre'] == selected_genre]\n",
    "    else:\n",
    "        filtered_df = df\n",
    "    \n",
    "    filtered_df = filtered_df[filtered_df['Rating'] >= selected_rating]\n",
    "    \n",
    "    return dash_table.DataTable(\n",
    "        id='table',\n",
    "        columns=[{'name': col, 'id': col} for col in filtered_df.columns],\n",
    "        data=filtered_df.to_dict('records')\n",
    "    )\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
